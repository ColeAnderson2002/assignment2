{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ColeAnderson2002/assignment2/blob/main/ColeAnderson_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling and Exploratory Data Analysis\n",
        "## Do Q1 and Q2, and one other question.\n",
        "`! git clone https://www.github.com/DS3001/assignment2`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWERED QUESTIONS: 1, 2, 5"
      ],
      "metadata": {
        "id": "H1O_aEfQOB2N"
      },
      "id": "H1O_aEfQOB2N"
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://www.github.com/DS3001/assignment2\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGjtEehh3BaG",
        "outputId": "ea206494-2a4f-4049-f0f0-7b5a82ccb8f0"
      },
      "id": "QGjtEehh3BaG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'assignment2'...\n",
            "warning: redirecting to https://github.com/DS3001/assignment2.git/\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 36 (delta 8), reused 5 (delta 5), pack-reused 24\u001b[K\n",
            "Receiving objects: 100% (36/36), 5.47 MiB | 23.04 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In accord with the abstract, it seems this paper emphasizes the importance of \"data tidying,\" in order to clean data itself that is messy and difficult to pull conclusions from at a bare level. It goes onto to explain the significance of specific structuring within tidying data, which is critical to the methodology of such.\n",
        "\n",
        "2. Tidy data standard's intention is to provide data scientists a way to organize and assign data values within a specific dataset that is complex/convoluted from the get go. Such makes intitial cleaning of data much easier, due to the fact that you don't need to start from the beginning every time. It \"facilitates initial exploration and analysis of the data, and to simiplify the development of data analysis tools that work well together,\" as the paper says itself!\n",
        "\n",
        "3. From my understanding, this first sentence is an analogy in that any dataset by outward perception is a \"dataset,\" but what composes it is what gives the dataset its unique identity. The same can be said about families—I have a family, just like my friend, but our families have different members, which is what makes us and our families unique! Looking at the second sentence, I suppose that it means one can rather easily distinguish/determine whether values are observations or variables, but then when you actually try to put a definition to them, it becomes challenging because there is so much vastness and variablity when it comes to the world of data and datasets—one can assume all datasets are different, in some way.\n",
        "\n",
        "4. Reading through section 2.2, Wickham defines the following:\n",
        "      - Values: these are what makes up a dataset, and can be quantitative or qualitative (numbers or strings)\n",
        "      - Variables: One way in which values are organized. These are values that have an underlying attribute across different units, like a age, height, etc.\n",
        "      - Observations: The second way values can be organized, in which they are measured on a shared unit. This can be something like information/data on a specific person, or something that happened on a certain/one day. It has one unit.\n",
        "5. In this section, Wickham literally says \"Tidy data is a standard way of mapping the meaning of a dataset to its structure,\" but he goes into further specifics. He explains that in tiny data, each variable formulates a column, obersvations forms rows, and each observational unit forms a table. Essentially, tiny data has those three tactics to clean (\"tidy\") data, and they serve different purposes but work together to help the user.\n",
        "\n",
        "6. According to Wickham, the 5 most common problems in messy datasets are as follows:\n",
        "      - Column headers are values, not variable names.\n",
        "      - Multiple variables are stored in one column.\n",
        "      - Variables are stored in both rows and columns.\n",
        "      - Multiple types of observational units are stored in the same table.\n",
        "      - A single observational unit is stored in multiple tables.\n",
        "\n",
        "\n",
        "  Looking at Table 4 and its data, I would argue that it is messy because the \"variables\" of the dataset are actually values themselves, in that they are quantitative numbers rather than being variable names. This follows his 5 most common problems! It can be reorganized as Wickham does later to make the dataset tidier.\n",
        "\n",
        "  \"Melting\" a dataset refers to turning columns into rows (\"making wide datasets long or tall\") which sees data, that is from separate columns, being stacked into their own rows. This has the dataset made into a \"tidier\" format.\n",
        "\n",
        "\n",
        "7. Table 11 is considered messy due the following: The data itself is portrayed in a way that makes it spread out across various columns, which in turn makes it difficult to understand. Further, the column names (like d1, d2...) are values, not the variable names. Variables are stored in both rows and columns that are the same information (in line with the 5 common problems). When we look at table 12, we are much happier (at least I am), as it is \"molten.\" There are specific variable names in their own columns. There are no missing data points like there are in table 11. Also, the overall display of the data is much neater, especially looking at the variables of tmax and tmin.\n",
        "\n",
        "8. The chicken and egg problem in tidy data: \" if tidy data is only as useful as the tools that work with it, then tidy tools will be inextricably linked to tidy data.\" Essentially, there are specific tidy tools that we use for tidy data... then what happens when changes to the data structures, or even these tools, occur? This would lead to a problem where efficient outcome and productivity can come to a halt, or hinderance. In turn, Wickham futuristically hopes that the framework he provided will lead to even better data storage strategeis, and also better tools for tidying and cleaning data. He also hopes for more human/user-based interaction and opportunity within the methodology of the field, and also bringing in more principles to \"guide the design of tidy data\" which reference \"statistical and cognitive factors.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "EV3R-abLYCqN"
      },
      "id": "EV3R-abLYCqN"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wjOgdHwdud7Y"
      },
      "id": "wjOgdHwdud7Y"
    },
    {
      "cell_type": "code",
      "source": [
        " # First step was to load these two packages, as I can use them for wrangling!\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iAedAsq2udmS"
      },
      "id": "iAedAsq2udmS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I am going to load the airbnb csv file\n",
        "#uploaded downloaded CSV to colab files, then copied path through there\n",
        "\n",
        "adf = pd.read_csv('/airbnb_hw.csv')\n",
        "sdf = pd.read_csv('/sharks (1).csv', low_memory=False)"
      ],
      "metadata": {
        "id": "RXfYIehUy6jG"
      },
      "id": "RXfYIehUy6jG",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 1 (## are notes, # is the code run)\n",
        "\n",
        "## First I want to view some specifics of the data\n",
        "\n",
        "# print(adf)\n",
        "# print(adf.info())\n",
        "# print(adf.head(20))\n",
        "# print(type('Price')) ## IT'S A STRING, SHOULD BE NUMBERIC\n",
        "\n",
        "\n",
        "## Taking out commas\n",
        "\n",
        "# adf['Price']= adf['Price'].str.replace(',','') ## THIS REMOVED THE ',' IN NUMBERS >999\n",
        "# print(adf['Price'].unique()) ## PROOF OF ABOVE\n",
        "# print(adf['Price'].value_counts()) ## ^\n",
        "\n",
        "adf['Price'] = pd.to_numeric(adf['Price'], errors='coerce') # Coerce the variable to numeric\n",
        "adf['Prices'] = np.log(adf['Price']) ## made a new column called Prices, which is the numeric\n",
        "\n",
        "print(adf['Prices'].dtypes) ## IT'S NUMERIC!\n",
        "adf['Prices'].hist(bins=50) ## using this as visualization\n",
        "\n",
        "# In all, I first wanted to just view the data, which I did with those initial lines. I then removed the commas, and then used coercion to convert the variable type to numeric. I also used the np.log from numpy, which I explained above because I made a new variable called \"prices\"\n"
      ],
      "metadata": {
        "id": "XanXSVFG6_cc"
      },
      "id": "XanXSVFG6_cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 2\n",
        "\n",
        "# print(sdf)\n",
        "# print(sdf.info())\n",
        "# print(sdf.head(20))\n",
        "# print(sdf['Type'])\n",
        "# print(sdf['Type'].unique()) ## SHOWS ME THE TYPES SPECIFICALLY\n",
        "# print(sdf['Type'].nunique()) # THIS SHOWS ME THE NUMBER OF UNIQUE VALUES WITHIN TYPE (THERE'S ONLY 12 OF THIS... NECESSARY DATA?)\n",
        "\n",
        "# sdf['Type'] = sdf['Type'].replace('nan', np.nan) # USING NUMPY, HAVE REPLACED THE STRING 'nan' WITH NaN VALUES – MAKES IT EASIER TO CLEAN/SEE DATA\n",
        "# sdf['Type'].fillNA(np.nan) ## COULD ALSO HAVE USED THIS FOR THE ABOVE\n",
        "\n",
        "# sdf['Type'+'_nan'] = sdf['Type'].isnull() # THIS CREATES A MISSING VALUE DUMMY, AND SHOWS THAT I DO HAVE MISSING\n",
        "\n",
        "# print('Missing Values: \\n', sum(sdf['Type'+'_nan']),'\\n') ## HAVE 5 TOTAL MISSING\n",
        "\n",
        "# sdf = sdf.dropna(subset=['Type']) ## THIS WAS USED, AND NEW TO ME, TO ESSENTAILLY DROP THE 5 REMAINING MISSING\n",
        "\n",
        "# print('Missing Values: \\n', sum(sdf['Type'+'_nan']),'\\n') ## NOW HAVE ZERO MISSING!\n",
        "\n",
        "\n",
        "# What I did: replaced string 'nan' with actual NaNs, created a dummy for missing values, and then removed rows with missing 'Type' values, ensuring no missing values were left.\n"
      ],
      "metadata": {
        "id": "4Jfq8XgI7Qa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749a43f6-4c17-4c47-a596-e006b2e6311d"
      },
      "id": "4Jfq8XgI7Qa3",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values: \n",
            " 0 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 3\n",
        "\n",
        "pdf = pd.read_csv('/content/VirginiaPretrialData2017.csv')\n",
        "\n",
        "# print(pdf.shape, '\\n')\n",
        "# print(pdf.dtypes, '\\n')\n",
        "# print(pdf.columns[1:5], '\\n')\n",
        "# print(pdf.head(10))\n",
        "\n",
        " ## Simply to view some of the data (above)\n",
        "\n",
        "pdf['WhetherDefendantWasReleasedPretrial' + '_nan'] = pdf['WhetherDefendantWasReleasedPretrial'].isnull() # PUTS NULL VALUES INTO COLUMN\n",
        "\n",
        "print(\"Missing Values: \", sum(pdf['WhetherDefendantWasReleasedPretrial_nan'])) # THIS SHOWS THERE ARE ZERO\n",
        "\n",
        "print(pdf['WhetherDefendantWasReleasedPretrial'].nunique()) # number of unique, 901\n",
        "print(pdf['WhetherDefendantWasReleasedPretrial'].unique()) # 9 (Unclear), 0 (Not released), 1 (Released) (see page 13 of codebook)\n",
        "\n",
        "# Truthfully, I cannot find anything that \"messy\" or needing of altering. I didn't find any \"missing data\" values. I viewed the data, and found those observations, and figured that nothing to my capabilities could clean it more.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlL8nQOpi4Eq",
        "outputId": "65c8b7c4-29ad-443d-8fc4-2fb994216f58"
      },
      "id": "xlL8nQOpi4Eq",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-133-8868d67e872e>:3: DtypeWarning: Columns (1,4,5,7,79,80,81,82,83,84,108,163,164,165,166,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,270,271,272,273,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,301,302,303,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,706) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pdf = pd.read_csv('/content/VirginiaPretrialData2017.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values:  0\n",
            "3\n",
            "[9 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 4\n",
        "\n",
        "# print(pdf['ImposedSentenceAllChargeInContactEvent'])\n",
        "# print(pdf['SentenceTypeAllChargesAtConvictionInContactEvent'])\n",
        "\n",
        "# print(pdf['ImposedSentenceAllChargeInContactEvent'].unique()) ## FIRST VALUE IS EMPTY\n",
        "\n",
        "pdf['ImposedSentenceAllChargeInContactEvent'] = pdf['ImposedSentenceAllChargeInContactEvent'].replace(' ','0') ## NOW THE FIRST VALUE ISN'T EMPTY\n",
        "# print(pdf['ImposedSentenceAllChargeInContactEvent'].unique())\n",
        "\n",
        "pdf['ImposedSentenceAllChargeInContactEvent'] = pd.to_numeric(pdf['ImposedSentenceAllChargeInContactEvent']) ## MADE ALL OF THE VALUES NUMERIC\n"
      ],
      "metadata": {
        "id": "5V9aECV_kg25"
      },
      "id": "5V9aECV_kg25",
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c11bcd96-2834-41a4-80fe-d354b4277fd9",
      "metadata": {
        "id": "c11bcd96-2834-41a4-80fe-d354b4277fd9"
      },
      "source": [
        "**Q3.** This question provides some practice doing exploratory data analysis and visualization.\n",
        "\n",
        "The \"relevant\" variables for this question are:\n",
        "  - `level` - Level of institution (4-year, 2-year)\n",
        "  - `aid_value` - The average amount of student aid going to undergraduate recipients\n",
        "  - `control` - Public, Private not-for-profit, Private for-profit\n",
        "  - `grad_100_value` - percentage of first-time, full-time, degree-seeking undergraduates who complete a degree or certificate program within 100 percent of expected time (bachelor's-seeking group at 4-year institutions)\n",
        "\n",
        "1. Load the `./data/college_completion.csv` data with Pandas.\n",
        "2. What are are the dimensions of the data? How many observations are there? What are the variables included? Use `.head()` to examine the first few rows of data.\n",
        "3. Cross tabulate `control` and `level`. Describe the patterns you see.\n",
        "4. For `grad_100_value`, create a histogram, kernel density plot, boxplot, and statistical description.\n",
        "5. For `grad_100_value`, create a grouped kernel density plot by `control` and by `level`. Describe what you see. Use `groupby` and `.describe` to make grouped calculations of statistical descriptions of `grad_100_value` by `level` and `control`. Which institutions appear to have the best graduation rates?\n",
        "6. Create a new variable, `df['levelXcontrol']=df['level']+', '+df['control']` that interacts level and control. Make a grouped kernel density plot. Which institutions appear to have the best graduation rates?\n",
        "7. Make a kernel density plot of `aid_value`. Notice that your graph is \"bi-modal\", having two little peaks that represent locally most common values. Now group your graph by `level` and `control`. What explains the bi-modal nature of the graph? Use `groupby` and `.describe` to make grouped calculations of statistical descriptions of `aid_value` by `level` and `control`.\n",
        "8. Make a scatterplot of `grad_100_value` by `aid_value`. Describe what you see. Now make the same plot, grouping by `level` and then `control`. Describe what you see. For which kinds of institutions does aid seem to increase graduation rates?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98d34a3b-c21d-4dc9-a8d2-fb7686804ceb",
      "metadata": {
        "id": "98d34a3b-c21d-4dc9-a8d2-fb7686804ceb"
      },
      "source": [
        "**Q4.** This question uses the Airbnb data to practice making visualizations.\n",
        "\n",
        "  1. Load the `./data/airbnb_hw.csv` data with Pandas. You should have cleaned the `Price` variable in question 2, and you'll need it later for this question.\n",
        "  2. What are are the dimensions of the data? How many observations are there? What are the variables included? Use `.head()` to examine the first few rows of data.\n",
        "  3. Cross tabulate `Room Type` and `Property Type`. What patterns do you see in what kinds of rentals are available? For which kinds of properties are private rooms more common than renting the entire property?\n",
        "  4. For `Price`, make a histogram, kernel density, box plot, and a statistical description of the variable. Are the data badly scaled? Are there many outliers? Use `log` to transform price into a new variable, `price_log`, and take these steps again.\n",
        "  5. Make a scatterplot of `price_log` and `Beds`. Describe what you see. Use `.groupby()` to compute a desciption of `Price` conditional on/grouped by the number of beds. Describe any patterns you see in the average price and standard deviation in prices.\n",
        "  6. Make a scatterplot of `price_log` and `Beds`, but color the graph by `Room Type` and `Property Type`. What patterns do you see? Compute a description of `Price` conditional on `Room Type` and `Property Type`. Which Room Type and Property Type have the highest prices on average? Which have the highest standard deviation? Does the mean or median appear to be a more reliable estimate of central tendency, and explain why?\n",
        "  7. We've looked a bit at this `price_log` and `Beds` scatterplot. Use seaborn to make a `jointplot` with `kind=hex`. Where are the data actually distributed? How does it affect the way you think about the plots in 5 and 6?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q5.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. From an overall standpoint, the U.S. Census Bureau (the federal group that is in charge of the Census) collects data through different sources: 1) from respondents who answer directly (which includes businesses), through surveys and censuses they send out/carry out. 2) The Bureau collects further data from other — usually primary — sources, such as federal data, state data, and local government data. They also draw on some commerical entities. These all are generally called \"administrative data\" because they are used to run programs and provide services to the public and for the public good. Additionally, the Census Bureau actually obtains and reuses data that is already existent from other government agencies, as it \"reduces costs\" and \"reduces the burden\" on those who actually respond to the census and surveys sent out.\n",
        "\n",
        "  More specifically in regards to collecting data on race, the Census Bureau gathers race data from self-identification, and in adherence to the 1997 United States Office of Management and Budget standards, where two separate race and ethnicity questions are offered. Ethnicity is asked about first (Hispanic or Latino (or) Not Hispanic or Latino). There are then \"five minimum\" categories for data on race: 1) American Indian or Alaska Native, 2) Asian, 3) Black or African American, 4) Native Hawaiian or Other Pacific Islander, 5) White. The 2020 Census actually saw some changes and new offerings, where people could self-identify as \"Some Other Race.\" The Bureau also states that they don't tell people how to identify, and that people have their own choice.\n",
        "\n",
        "\n",
        "2. From my understanding and thought, this data is gathered so that we can better society. To be more specific, racial identity is crucial in many different ways to the prosperity of our nation. This notion can apply to various purposes, such as, but not limited to, representation, policy-making, and having a general understanding of demographic contextulizations, which is all so critical in a thriving democracy. We want to be able to have accurate and beneficial understandings of the makeup of our country so that we can have an equitable and just place to live. Quality data is so important to this matter because if race is misrepresented, then we truly cannot provide a safe and just place to live and prosper for the citizens of the country. If there is bad quality data, then misconceptions and biases will tend to form, which are obviously negative.\n",
        "\n",
        "3. In my opinion, the Bureau does things well and others not. What was done well is the fact that this is a \"self-identifying\" based census, which means that there is no forced answers, thus the data should be rather representative in the sense that people are willingly answering (and hopefully truthfully, which presents a problem if not, that must be considered). I think they also do a good job in reaching out to different sources, in that they don't solely rely on surveys sent out, but rather pull from federal and state databases, meaning that their data is further representative of the population than it would be solely using surveys. The Bureau missed on some points. I feel like they still probably underrepresent minority groups, in that technological access to these sources aren't always provided to minority groups and those who cannot afford them. This probably enforces the Bureau to make estimates or rely on old data. In this light, the Census Bureau should reach out to more local authorities (governments, mayors, etc.) to get more data on their constituents. I think this can be very beneficial to reach these underepresented groups! With all of this, however, I think other data-collecting groups could follow the guidelines and principles of the Census Bureau. They do a very hard job in a very realistic and representative way, in my opinion. It is almost impossible to cover the 330,000,000 + population that we have in the United States, and their efforts to make their work more inclusive and understanding of the nation's population as a whole is admirable.\n",
        "\n",
        "4. The U.S. Census Bureau collects data based on sex and gender using their similar survey tactics and reaching out to other government-oriented groups. On the 2020 form, sex was simply either \"Male or Female\" and you'd check a box. It doesn't have a \"gender\" composition for the 2020 makeup, and is simply confined to those two categories, which I think is inadequate. However, in July 2021, they added new questions to the survey, such as \"your birth-assigned sex\" and \"currently described sex\" (self-identified, male, female, or transgender). They also just added a sexual orientation question as well in 2021, which was well overdue in my opinion. With that, it seems that constructive criticism has seemed to catch up without my input! I'm glad to see that the Bureau expanded their questions to include these groups/identifications, and I would've critiqued the 2020 Bureau for not having such. Nonetheless, self-surveying individuals will always be underepresentative, due to the fact that not everyone can or will respond.\n",
        "\n",
        "5. Looking at this question about cleaning data, I do have some concerns. For one, protective cleaning can lead to bias and misrepresentation, in that it limits the ability for individual identification, as \"data cleaners\" working for the Census Bureau have to eliminate some outliers most likely. With this, additional diversity groups are probably being left out in regard to gender. When there are missing values, I'd imagine that some sort of skewedness will lead to inaccurate conclusions from the data they collect and clean, misrepresenting the people. Some good practices that can be adopted are publishing how the data collected was cleaned to the public, and especially making their website easier to access! The Bureau also needs to do a better job about representing protected characteristics and genders, as does everyone. In regards to bad practices, there is a chance that people generalize and categorize certain protected genders into one category, which is again misrepresentative. The data cleaners in the Bureau probably have to skip a ton of information and missing values as well due to the essence of time and believing that what they are doing is for the better accuracy of the census, or even that they are being told to by upper-management and are scared for the welfare/status as an employee.\n",
        "\n",
        "6. Imputing values through an algorithm proposes many different problems. For one, it is an ethical threat, in the sense that assuming one's identity, gender, or sex, is immoral and unethical. Since the Census Bureau is a self-identifying survey, there should be no algorithm to eliminate or lessen the representation of the population, regardless of the complexities posed. This also can be explained for the notion of bias, and even accuracy as an algorithm can provide inaccurate misrepresentations of the population. Overall, the ethical concerns are the most prominent in my opinion, though the other points are surely critical."
      ],
      "metadata": {
        "id": "6jTcQejToZjO"
      },
      "id": "6jTcQejToZjO"
    },
    {
      "cell_type": "markdown",
      "id": "2f38f2fd-6381-481d-bba9-017f3d363426",
      "metadata": {
        "id": "2f38f2fd-6381-481d-bba9-017f3d363426"
      },
      "source": [
        "**Q6.** Open the `./data/CBO_data.pdf` file. This contains tax data for 2019, explaining where the money comes from that the U.S. Federal Government Spends in terms of taxation on individuals/families and payroll taxes (the amount that your employer pays in taxes on your wages).\n",
        "\n",
        "For some context, the Federal government ultimately spent about $4.4 trillion in 2019, which was 21% of GDP (the total monetary value of all goods and services produced within the United States). Individual Income Taxes is the amount individuals pay on their wages to the Federal government, Corporate Income Taxes is the taxes individuals pay on capital gains from investment when they sell stock or other financial instruments, Payroll Taxes is the tax your employer pays on your wages, Excises and Customs Duties are taxes on goods or services like sin taxes on cigarettes or alcohol, and Estate and Gift Taxes are taxes paid on transfers of wealth to other people.\n",
        "\n",
        "1. Get the Millions of Families and Billions of Dollars data into a .csv file and load it with Pandas.\n",
        "2. Create a bar plot of individual income taxes by income decile. Explain what the graph shows. Why are some values negative?\n",
        "3. Create a bar plot of Total Federal Taxes by income decile. Which deciles are paying net positive amounts, and which are paying net negative amounts?\n",
        "4. Create a stacked bar plot for which Total Federal Taxes is grouped by Individual Income Taxes, Payroll Taxes, Excises and Customs Duties, and Estate and Gift Taxes. How does the share of taxes paid vary across the adjusted income deciles? (Hint: Are these the kind of data you want to melt?)\n",
        "5. Below the Total line for Millions of Families and Billions of Dollars, there are data for the richest of the richest families. Plot this alongside the bars for the deciles above the Total line. Describe your results.\n",
        "6. Get the Percent Distribution data into a .csv file and load it with Pandas. Create a bar graph of Total Federal Taxes by income decile.\n",
        "7. A tax system is progressive if higher-income and wealthier individuals pay more than lower-income and less wealthy individuals, and it is regressive if the opposite is true. Is the U.S. tax system progressive in terms of amount paid? In terms of the percentage of the overall total?\n",
        "8. Do the rich pay enough in taxes? Defend your answer."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}